# Proyecto Final 2025-1: Pong AI
## **CS2013 Programaci√≥n III** ¬∑ Informe Final

### **Descripci√≥n**

Implementaci√≥n de una red neuronal multicapa en C++ desde cero para controlar un agente que juega Pong. El sistema aplica aprendizaje por refuerzo con SARSA, arquitectura modular de capas, normalizaci√≥n de entradas, exploraci√≥n Œµ-greedy y entrenamiento supervisado. No se utiliz√≥ ninguna librer√≠a de machine learning externa.

---

### Contenidos

1. [Datos generales](#datos-generales)
2. [Requisitos e instalaci√≥n](#requisitos-e-instalaci√≥n)
3. [Investigaci√≥n te√≥rica](#1-investigaci√≥n-te√≥rica)
4. [Dise√±o e implementaci√≥n](#2-dise√±o-e-implementaci√≥n)
5. [Ejecuci√≥n](#3-ejecuci√≥n)
6. [An√°lisis del rendimiento](#4-an√°lisis-del-rendimiento)
7. [Trabajo en equipo](#5-trabajo-en-equipo)
8. [Conclusiones](#6-conclusiones)
9. [Bibliograf√≠a](#7-bibliograf√≠a)
10. [Licencia](#licencia)

---

### Datos generales

* **Tema**: Redes Neuronales para Aprendizaje por Refuerzo
* **Grupo**: `j3ingineers`
* **Integrantes**:

    * Diego Antonio Escajadillo Guerrero - 201910150

---

### Requisitos e instalaci√≥n

1. **Compilador**: GCC 11 o superior / MSVC 2019 con C++17
2. **Dependencias**:

    * CMake 3.18+
    * Ninguna otra librer√≠a externa

3. **Instalaci√≥n**:

   ```bash
   git clone https://github.com/usuario/proyecto-pong-ai.git
   cd proyecto-pong-ai
   mkdir build && cd build
   cmake ..
   make
   ```

---

### 1. Investigaci√≥n te√≥rica

* Historia de las redes neuronales
* Aprendizaje por refuerzo: SARSA vs Q-Learning
* Exploraci√≥n-explotaci√≥n: pol√≠tica Œµ-greedy
* Normalizaci√≥n de entradas y funciones de activaci√≥n (ReLU)

---

### 2. Dise√±o e implementaci√≥n

#### 2.1 Arquitectura modular

* Capas como clases que heredan de `ILayer`
* Optimizaci√≥n mediante `SGD`
* Entrenamiento on-policy con `learnOnPolicy()`

#### 2.2 Organizaci√≥n del proyecto

```
Pong_AI/
‚îú‚îÄ‚îÄ include/utec/nn/         # Capas, red y optimizadores
‚îú‚îÄ‚îÄ include/utec/agent/      # Agente SARSA y entorno Pong
‚îú‚îÄ‚îÄ src/                     # Implementaci√≥n
‚îú‚îÄ‚îÄ main.cpp                 # Entrenamiento completo
```

#### 2.3 Casos de prueba

* Test de forward y backward en `Dense`
* Evaluaci√≥n de convergencia del agente
* Validaci√≥n del modelo entrenado (`test_agent_env.cpp`)

---

### 3. Ejecuci√≥n

1. Compilar el proyecto
2. Ejecutar entrenamiento:
   ```bash
   ./Pong_AI
   ```
3. Ejecutar evaluaci√≥n del modelo:
   ```bash
   ./test_agent_env
   ```
4. Analizar resultados:
    * `pesos.txt`: pesos del modelo
    * `winrate.csv`: desempe√±o por bloques de entrenamiento

---

### 4. An√°lisis del rendimiento

* **Episodios entrenados**: 3000
* **Duraci√≥n aprox.**: ~5 sec
* **Modelo final**: arquitectura 3‚Äì16‚Äì8‚Äì1 con activaciones ReLU
* **Winrate evaluado**: 35‚Äì45% promedio sin replay buffer
* **Observaciones**:
    * üü¢ Entrena sin dependencias externas
    * üî¥ Limitado por no usar experiencia acumulada
* **Mejoras futuras**:
    * Implementar Q-Learning off-policy
    * Agregar replay buffer y batch training
    * Exportar m√©tricas y visualizar en Python

* **Video de Demostraci√≥n**:
 * https://youtu.be/ksVcA2Vv55g
---

### 5. Trabajo en equipo

| Tarea                        | Miembro       | Rol                           |
|-----------------------------|----------------|--------------------------------|
| Investigaci√≥n te√≥rica       | Diego Escajadillo     | Documentar teor√≠a RL y NNs     |
| Dise√±o y normalizaci√≥n      |                 | Arquitectura + entradas        |
| Implementaci√≥n de la red    |                 | Clases Dense, ReLU, NN         |
| Entrenamiento SARSA         |                 | Agente y l√≥gica de aprendizaje |
| Pruebas y winrate.csv       |                 | Registro y visualizaci√≥n       |
| Documentaci√≥n final         |                    | README, licencias y demo       |

---

### 6. Conclusiones

* Se logr√≥ implementar un sistema completo de RL con SARSA desde cero.
* El agente aprende parcialmente, aunque el rendimiento es limitado por falta de memoria.
* Aprendimos a integrar teor√≠a, entrenamiento supervisado y refuerzo.
* El proyecto es una buena base para evolucionar hacia DQN o PPO.

---

### 7. Bibliograf√≠a

[1] R. S. Sutton and A. G. Barto, *Reinforcement Learning: An Introduction*, MIT Press, 2018.  
[2] I. Goodfellow, Y. Bengio, and A. Courville, *Deep Learning*, MIT Press, 2016.  
[3] F. Chollet, *Deep Learning with Python*, Manning, 2017.  
[4] D. Silver et al., "Mastering the game of Go with deep neural networks and tree search," *Nature*, vol. 529, 2016.

---

### Licencia

Este proyecto usa la licencia **MIT**. Ver [LICENSE](LICENSE) para detalles.
